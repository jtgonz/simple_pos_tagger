{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging in Python\n",
    "\n",
    "Recently, I came across Professor Michael Collins's [collection of lectures](http://www.cs.columbia.edu/~mcollins/) on  statistical natural language processing. Inspired by his first two chapters, I've put together a simple part-of-speech tagger that incorporates many of the concepts covered.\n",
    "\n",
    "All functions are available on GitHub, [here](https://github.com/jtgonz/simple_pos_tagger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "**Read in a sentence, and assign the correct part of speech (noun, verb, etc.) to each word.**\n",
    "\n",
    "Say we have a sequence of words:\n",
    "$$\\newcommand{\\coeff}[2]{%\n",
    "  \\underset{\\displaystyle\\mathstrut #2}{\\strut #1}%\n",
    "}\n",
    "\\coeff{\\text{The}}{x_1}\\ \\coeff{\\text{big}}{x_2}\\ \\coeff{\\text{dog}}{x_3}\\ \\coeff{\\text{barks.}}{x_4}$$\n",
    "\n",
    "We also have a sequence of part-of-speech tags:\n",
    "$$\\newcommand{\\coeff}[2]{%\n",
    "  \\underset{\\displaystyle\\mathstrut #2}{\\strut #1}%\n",
    "}\n",
    "\\coeff{\\text{DT}}{y_1}\\ \\coeff{\\text{JJ}}{y_2}\\ \\coeff{\\text{NN}}{y_3}\\ \\coeff{\\text{VB}}{y_4}\\ \\coeff{\\text{STOP}}{y_5}$$\n",
    "<br>\n",
    "Our tagger will model the probabliity that the sequence of words $x_1...x_n$ is seen with the sequence of part-of-speech tags $y_1...y_{n+1}$. Then, given a sequence of words, we can use the model to choose the sequence of tags that maximizes that probability $p$.\n",
    "\n",
    "\n",
    "This tagger takes the form of a **Hidden Markov Model**, shown below.\n",
    "<p>\n",
    "\n",
    "$$p(x_1...x_n,y_1...y_{n+1}) = \\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1}) \\prod_{i=1}^ne(x_i|y_i)$$\n",
    "\n",
    "The first product is a second-order Markov sequence. Here, we're making the the assumption that each part-of-speech tag depends on the two preceding tags (and only on the two preceeding tags). This probability is given by our **transition parameter** $q(s|u,v)$. \n",
    "\n",
    "The second product represents the probability of seeing a certain word given a certain tag. This is given by our **emission parameter** $e(x|s)$.\n",
    "\n",
    "We need to estimate both of these parameters. To do so, we'll use training data from the CoNLL-2000 corpus, a collection of articles from the Wall Street Journal. Both the training and the test data are available for free [here](http://www.cnts.ua.ac.be/conll2000/chunking/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data\n",
    "\n",
    "Before building anything, we'll split the given training data (\"train.txt\") into two sets. We'll use the first set (\"train_data.txt\") to calculate our transition and emission parameters. Later, we'll use the second set (\"dev_data.txt\") as development data to create some additional smoothing parameters. The final split looks like this:\n",
    "\n",
    "- **train_data.txt** (2.1 MB, from train.txt)\n",
    "- **dev_data.txt** (723 KB, from train.txt)\n",
    "- **test.txt** (639 KB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and open output files\n",
    "train_file = open('train_data.txt', 'w+')\n",
    "dev_file = open('dev_data.txt', 'w+')\n",
    "\n",
    "# split train.txt into train_data.txt and dev_data.txt\n",
    "# this will be a 75/25 split\n",
    "sentence = ''\n",
    "with open('train.txt') as corpus:\n",
    "    for i,line in enumerate(corpus):\n",
    "        sentence += line\n",
    "        if line == '\\n':\n",
    "            train_file.write(sentence) if i % 4 else dev_file.write(sentence)\n",
    "            sentence = ''\n",
    "            \n",
    "# close output files\n",
    "train_file.close()\n",
    "dev_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get frequency counts\n",
    "\n",
    "Our next step will be to examine the training data, and count how often certain patterns appear. In particular, we're looking for:\n",
    "\n",
    "- **ngram_counts** - How often we see certain tag sequences in the training data\n",
    "- **emiss_counts** - How often we see certain words in the training data\n",
    "\n",
    "A more detailed explanation follows the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_all_counts(infile, n=3):\n",
    "    \"\"\"\n",
    "    Iterate through corpus, get n-gram counts and emission counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # create dictionaries to store ngram and emission counts\n",
    "    ngram_counts = defaultdict(lambda: 0)\n",
    "    emiss_counts = defaultdict(lambda: 0)\n",
    "\n",
    "    # initialize tag list to ['*','*','*',...]\n",
    "    tag_list = ['*'] * n\n",
    "\n",
    "    with open(infile) as corpus:\n",
    "        for line in corpus:\n",
    "\n",
    "            # reached the end of a sentence\n",
    "            if line == '\\n':\n",
    "\n",
    "                # if there is no data, just skip\n",
    "                if tag_list[-1] == '*': continue\n",
    "\n",
    "                word = False\n",
    "                tag = 'STOP'\n",
    "\n",
    "            # get word and tag from line\n",
    "            else:\n",
    "                word, tag, chunk = line.rstrip().split(' ')\n",
    "\n",
    "            # add new tag to end of tag list\n",
    "            tag_list.pop(0)\n",
    "            tag_list.append(tag)\n",
    "\n",
    "            # increment count by 1 when ngram appears in corpus\n",
    "            for i in xrange(1,n+1):\n",
    "                tag_sequence = tuple(tag_list[-i:])\n",
    "                ngram_counts[tag_sequence] += 1\n",
    "\n",
    "            # increment emmision count by 1, or reset tag list if no more words\n",
    "            if word:\n",
    "                emiss_counts[tag, word] += 1\n",
    "                emiss_counts['_', word] += 1\n",
    "            else:\n",
    "                tag_list = ['*'] * n\n",
    "\n",
    "    return ngram_counts, emiss_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_counts, emiss_counts = get_all_counts('train_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a couple of useful things. The first value, `ngram_counts`, is a dictionary mapping n-grams to their frequency in the training data. For example:\n",
    "\n",
    "**How many times was an adjective (JJ) followed by a noun (NN) and then a past-tense verb (VBD)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts[('JJ', 'NN', 'VBD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many times did we see a verb followed by an adjective?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts[('VBD','JJ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How often did a sentence begin with a personal pronoun?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts[('*', '*','PRP')] # could also use ngram_counts[('*','PRP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have information on the conditional frequencies of words in the training data. The second value, `emiss_counts`, tells us how many times we saw a certain word, given a certain part-of-speech tag. For example:\n",
    "\n",
    "**How many times did we see the word \"charge\" used as a noun?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emiss_counts[('NN', 'charge')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many times did we see the word \"charge\" used as a verb?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emiss_counts[('VB', 'charge')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline tagger\n",
    "\n",
    "With this basic information, we can put together a simple tagger to use as a baseline. For each word, this tagger will choose the part of speech most frequently associated with that word in the training data. For example, the word \"charge\" will always be labeled as a noun (see example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_baseline(word_seq, e_counts, tag_set):\n",
    "    return [max({(tag,word):emiss_counts[tag,word] \\\n",
    "                 for tag in tag_set}.iteritems(), key=lambda x:x[1])[0][0] for word in word_seq]\n",
    "\n",
    "def get_unique_unigrams(ngram_counts):\n",
    "    return {''.join(ngram) for ngram in ngram_counts if len(ngram) == 1 and ngram_counts[ngram]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the baseline tagger like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DT', 'NN', 'VBD', 'IN', 'DT', 'NNP']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set = get_unique_unigrams(ngram_counts)\n",
    "sentence = ['the', 'dog', 'went', 'into', 'the', 'house']\n",
    "\n",
    "run_baseline(sentence, emiss_counts, tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the tagger\n",
    "\n",
    "We'll use three metrics to measure the effectiveness of our tagger.\n",
    "\n",
    "1. **Recall.** Out of all the test words, how many were we able to tag?\n",
    "2. **Precision.** Out of all the words we tagged, how many did we tag correctly?\n",
    "3. **F-Score.** The geometric mean of precision and recall.\n",
    "\n",
    "Below, we've defined a function that takes a tagger and test corpus as input, and outputs each of these three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def score_tagger(infile, tagger, *params):\n",
    "    \"\"\"\n",
    "    description goes here\n",
    "    Precision: # of correctly tagged words / # of tagged words\n",
    "    Recall: # of tagged words / # of words\n",
    "    \"\"\"\n",
    "    \n",
    "    # store counts of tagged words\n",
    "    num_correct_words = num_tagged_words = num_words = 0\n",
    "\n",
    "    with open(infile) as corpus:\n",
    "        word_seq = []\n",
    "        answer = []\n",
    "        \n",
    "        for line in corpus:\n",
    "            \n",
    "            if line == '\\n':\n",
    "\n",
    "                # if there is no data, just skip\n",
    "                if word_seq == []: continue\n",
    "\n",
    "                # tagger will return either a list of tags (ex. ['DT','NN','VB'])\n",
    "                # or ['*','*',...'*'] if sentence could not be tagged\n",
    "                result = tagger(word_seq, *params)\n",
    "                \n",
    "                # get number of correctly tagged words\n",
    "                num_correct_words += sum([result[i] == answer[i] for i in xrange(len(answer))])\n",
    "                num_tagged_words += len(result) if result[0] != '*' else 0\n",
    "                num_words += len(result)\n",
    "\n",
    "                # reset sentence\n",
    "                word_seq = []\n",
    "                answer = []\n",
    "\n",
    "                continue\n",
    "\n",
    "            # get word and tag, add to list\n",
    "            word, tag, chunk = line.rstrip().split(' ')\n",
    "            word_seq.append(word)\n",
    "            answer.append(tag)\n",
    "\n",
    "    # calculate precision and recall\n",
    "    precision_words = num_correct_words/num_tagged_words\n",
    "    recall_words = num_tagged_words/num_words\n",
    "    fscore_words = 2*precision_words*recall_words/(precision_words+recall_words)\n",
    "    \n",
    "    return precision_words, recall_words, fscore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8043776516030986, 1.0, 0.8915845869499099)"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_tagger('test.txt', run_baseline, emiss_counts, tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline tagger does a decent job, assigning a label to every word and correctly labeling around 80% of words in the corpus. Now that we have an idea of where we're starting from, we can work on building the Hidden Markov Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get maximum likelihood estimates\n",
    "\n",
    "We can use our obtained `ngram_counts` and `emiss_counts` to calculate maximum likelihood estimates for the transition and emission parameters. These are defined as:\n",
    "\n",
    "$$q_{ML}(s|u,v) = \\frac{c(u,v,s)}{c(u,v)} =\n",
    "\\frac{\\texttt{ngram_counts[u,v,s]}}{\\texttt{ngram_counts[u,v]}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$e_{ML}(x|s) = \\frac{c(s \\rightarrow x)}{c(s)} =\n",
    "\\frac{\\texttt{emiss_counts[s,x]}}{\\texttt{ngram_counts[s]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_unique_unigrams(ngram_counts):\n",
    "    return sum(ngram_counts[ngram] and len(ngram) == 1 for ngram in ngram_counts) - 1\n",
    "\n",
    "def count_total_words(emiss_counts):\n",
    "    return sum(key[0] == '_' and emiss_counts[key] for key in emiss_counts)\n",
    "\n",
    "def get_ml_estimates(ngram_counts, emiss_counts):\n",
    "    \"\"\"\n",
    "    Get maximum likelihood estimates for transition and emission\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # create dictionaries to store maximum likelihood estimates\n",
    "    # for transition and emission parameters\n",
    "    qml_est = defaultdict(lambda: 0)\n",
    "    eml_est = defaultdict(lambda: 0)\n",
    "\n",
    "    num_unique_tags = count_unique_unigrams(ngram_counts)\n",
    "    num_total_words = count_total_words(emiss_counts)\n",
    "\n",
    "    # get maximum likelihood estimates for transitions\n",
    "    # qml_est[(u,v,s)] = q_ml(s|u,v) = count(u,v,s) / count(u,v)\n",
    "    for ngram in ngram_counts.keys():\n",
    "\n",
    "        if len(ngram) > 1:\n",
    "            qml_est[ngram] = \\\n",
    "            ngram_counts[ngram] / (ngram_counts[ngram[:-1]] or ngram_counts[('STOP',)])\n",
    "        else:\n",
    "            qml_est[ngram] = ngram_counts[ngram] / num_unique_tags\n",
    "\n",
    "    # get maximum likelihood estimates for emissions\n",
    "    # eml_est[s][x] = e_ml(x|s) = count(s -> x) / count(s)\n",
    "    for key in emiss_counts:\n",
    "        eml_est[key] = emiss_counts[key] / (ngram_counts[(key[0],)] or num_total_words)\n",
    "\n",
    "    return qml_est, eml_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qml_est, eml_est = get_ml_estimates(ngram_counts, emiss_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll use these estimates (`qml_est` and `eml_est`) as our transition and emission parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi algorithm with maximum likelihood estimates\n",
    "\n",
    "Now that we've got estimates for our transition and emission parameters, how do we find the most likely tag sequence?\n",
    "\n",
    "One method would be examine all possible tag sequences for a given sentence, and calculate $p(x_1...x_n,y_1...y_{n+1})$ for each sequence. This method, however, is very inefficient. With $S$ possible tags for each word, and $n$ words in the sentence, we'd have to calculate $S^n$ probabilities and compare them all.\n",
    "\n",
    "The Viterbi algorithm works much quicker, solving this problem in $nS^{k+1}$ time (where $k$ is the order of the Markov sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def run_viterbi(word_seq, q_params, e_params, tag_set, use_pseudo=False, order=2):\n",
    "    \"\"\"\n",
    "    Implementation of the Viterbi algorithm with backpointers. Returns\n",
    "    the tags that maximize the probability of a given sentence occuring,\n",
    "    based on an n-gram hidden markov model.\n",
    "\n",
    "    Args:\n",
    "        q_params (defaultdict): Transition probabilities. q(s|u,v) -> q_params(u,v,s)\n",
    "        e_params (defaultdict): Emission probabilities. e(x|s) -> e_params(s,x)\n",
    "        word_seq (list): The sentence to be tagged.\n",
    "        tag_set (set): Set of potential tags for a word.\n",
    "        order (int): The order of the Markov sequence. Defaults to 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # base case for pi parameters, initialize backpointers\n",
    "    pi_params = {(0,('*',) * n):1 for n in xrange(1,order+1)}\n",
    "    bp_params = {}\n",
    "    \n",
    "    # probability words are seen with tags\n",
    "    sent_prob = 0\n",
    "\n",
    "    for k,word in enumerate(word_seq, start=1):\n",
    "\n",
    "        # create list of tag sets for words at position k-order+1 to k\n",
    "        tag_set_list = [tag_set if i+1 > 0 else {'*'} for i in xrange(k-order,k)]\n",
    "\n",
    "        # create set of all possible tag sequences that end at position k\n",
    "        tag_seq_list = {tag_seq for tag_seq in product(*tag_set_list)}\n",
    "        \n",
    "        # map unseen words to pseudo-words\n",
    "        if use_pseudo and e_params['_',word] == 0:\n",
    "            word = map_to_pseudo_word(word, k)\n",
    "\n",
    "        # iterate through tag sequences that end at position k\n",
    "        for tag_seq in tag_seq_list:\n",
    "\n",
    "            pi_params[k, tag_seq] = 0   # initialize pi parameter\n",
    "\n",
    "            # loop through set of tags in leftmost position\n",
    "            for tag in k-order > 0 and tag_set or {'*'}:\n",
    "\n",
    "                pi_key = k-1, (tag,) + tag_seq[:-1]     # looks like k-1,(w,u)\n",
    "                q_key = (tag,) + tag_seq                # looks like w,u,v -- transition param v|w,u\n",
    "                e_key = tag_seq[-1], word               # looks like v,x -- emission param x|v\n",
    "\n",
    "                temp = pi_params[pi_key] * q_params[q_key] * e_params[e_key]\n",
    "                \n",
    "                # get largest pi parameter and remember arguments\n",
    "                if temp > pi_params[k, tag_seq]:\n",
    "                    pi_params[k, tag_seq] = temp\n",
    "                    bp_params[k, tag_seq] = tag\n",
    "                    \n",
    "            if k == len(word_seq):\n",
    "\n",
    "                temp_sent_prob = pi_params[k, tag_seq] * q_params[tag_seq + ('STOP',)]\n",
    "\n",
    "                if temp_sent_prob > sent_prob:\n",
    "                    sent_prob = temp_sent_prob\n",
    "                    final_tag_seq = tag_seq\n",
    "    \n",
    "    # were we able to tag the sentence?\n",
    "    if not sent_prob:\n",
    "        return ['*'] * k\n",
    "    \n",
    "    # retrieve tags for each word in the sentence\n",
    "    final_tag_seq = list(final_tag_seq)\n",
    "    for k in xrange(k-order, 0, -1):\n",
    "        final_tag_seq.insert(0, bp_params[k+order, tuple(final_tag_seq[:order])])\n",
    "\n",
    "    return final_tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the tagger like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBD', 'PRP$', 'JJ', 'NN', '.']"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set = get_unique_unigrams(ngram_counts)\n",
    "sentence = ['Theodore', 'fed', 'his', 'pet', 'macaw', '.']\n",
    "\n",
    "run_viterbi(sentence, qml_est, eml_est, tag_set, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does it do on the test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger_1 = score_tagger('test.txt', run_viterbi, qml_est, eml_est, tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tagger        | F-Score       | Precision  | Recall  |\n",
    "| ------------- |------------:| ----:|----:|\n",
    "| Baseline      | 89.16% | 80.44% |100.0%|\n",
    "| Maximum Likelihood Estimates  | 6.27%      |  98.24% |3.24%|\n",
    "\n",
    "While we see a large jump in precision, there's an much more serious drop in recall. We're only able to find tags for around three percent of words, and the F-score suffers as a result.\n",
    "\n",
    "This is a consequence of sparse training data. If a word does not appear in the training corpus, then the emission parameter for that word, $e_{ML}(x|s)$, will be zero. Remember that our probability $p(x_1...x_n,y_1...y_{n+1})$ is a product of the emission and transition parameters for every word in the sentence. Any unseen word in a test sentence will cause the tag sequence probability to drop to zero, regardless of what the rest of the words in the sentence are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-word mapping\n",
    "\n",
    "One way to solve this problem is by mapping low-frequency words in the training data to a set of pseudo-words. Then, when we encounter a word in the test data that we've never seen before, we can map it to a pseudo-word and use the corresponding emission parameter. This has the effect of closing the vocabulary -- every word we could encounter in the test data will be seen at least once in the training data.\n",
    "\n",
    "I've used the mapping below for training this tagger. If a word occurs in the training data less than five times, it's changed to the approriate pseudo-word.\n",
    "\n",
    "| Criteria        | Pseudo-Word       |\n",
    "| ------------- |------------|\n",
    "| First word in sentence | `__firstWord__` |\n",
    "| All capital letters | `__allCaps__` |\n",
    "| Capital letter followed by period | `__capPeriod__` |\n",
    "| First letter is capitalized | `__initCap__` |\n",
    "| All letters uncapitalized | `__lowercase__` |\n",
    "| Begins with '\\$', contains numbers | `__currency__` |\n",
    "| Numeric value | `__number__` |\n",
    "| All other words | `__other__` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [this script](https://github.com/jtgonz/simple_pos_tagger/blob/master/replace_words.py) to create a new training corpus, with low-frequency words mapped to their appropriate psuedo-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python replace_words.py train_data.txt > train_data_replaced.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks like this:\n",
    "\n",
    "`Confidence in the pound is widely expected to take another share dive if trade figures for September, due for release tomorrow, fail to show a substantial improvement from July and August's near-record deficits.`\n",
    "\n",
    "**`__firstWord__`** `in the pound is widely expected to take another share` **`__lowercase__`** `if trade figures for September, due for release tomorrow,` **`__lowercase__`** `to show a substantial improvement from July and August's` **`__other__ __lowercase__`**`.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly updated training corpus (and a slight modification to `run_viterbi`), we can re-train and re-evaluate the tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This is called by run_viterbi when we set pseudo=True\n",
    "\n",
    "def map_to_pseudo_word(word, k):\n",
    "    \n",
    "    if k == 1:                                          # first word in sentence\n",
    "        pseudo = '__firstWord__'\n",
    "    elif re.match(r'[A-Z]+$', word):                    # organization\n",
    "        pseudo = '__allCaps__'\n",
    "    elif re.match(r'[A-Z]\\.$', word):                   # person name initial\n",
    "        pseudo = '__capPeriod__'\n",
    "    elif re.match(r'[A-Z]\\w*$', word):                  # capitalized word\n",
    "        pseudo = '__initCap__'\n",
    "    elif re.match(r'[a-z]\\w*$', word):                  # uncapitalized word\n",
    "        pseudo = '__lowercase__'\n",
    "    elif re.match(r'\\$[0-9][0-9,.]*$', word):           # monetary amount (dollars)\n",
    "        pseudo = '__currency__'\n",
    "    elif re.match(r'[0-9]+[0-9-/.,A-Za-z]*$', word):    # numeric value\n",
    "        pseudo = '__number__'\n",
    "    else:                                               # other\n",
    "        pseudo = '__other__'\n",
    "    \n",
    "    return pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_counts, emiss_counts = get_all_counts('train_data_replaced.txt')\n",
    "qml_est, eml_est = get_ml_estimates(ngram_counts, emiss_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with pseudo-words\n",
    "tagger_2 = score_tagger('test.txt', run_viterbi, qml_est, eml_est, tag_set, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tagger        | F-Score       | Precision  | Recall  |\n",
    "| ------------- |------------:| ----:|----:|\n",
    "| Maximum Likelihood Estimates  | 6.27%      |  98.24% |3.24%|\n",
    "| Maximum Likelihood Estimates w/ pseudo-words      | 86.90% | 92.71% |81.77%|\n",
    "\n",
    "This is a large improvement over the last tagger. But there is still more that can be done to boost recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted maximum likelihood estimates\n",
    "\n",
    "Sparse training data doesn't only affect the emission parameters -- it affects the transition parameters as well. They may be certain trigram tag sequences in the test data that we never see in the training data.\n",
    "\n",
    "One way to account for this is by using discounting methods. We'll define the discounted estimate as:\n",
    "\n",
    "$$q_D(w|u,v) = \\frac{c(u,v,w)-\\beta}{c(u,v)}\\text{, where } \\beta \\text{ is between 0 and 1}$$\n",
    "\n",
    "The intuition here is that with a pure ML estimate, we are likely overestimating trigrams that are seen in the training corpus and underestimating trigrams that are not seen. Discounting methods attempt to correct this by subtracting a small amount from our observed trigrams, and distributing that subtracted probability mass evenly among the unobserved trigrams.\n",
    "\n",
    "To choose the value for $\\beta$, we'll test the tagger on the development data and iterate through a few possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_discounted_estimates(ngram_counts, tag_set, beta=0.5):\n",
    "\n",
    "    qd_est = defaultdict(lambda: 0)\n",
    "    alpha = defaultdict(lambda: 0)\n",
    "\n",
    "    # get discounted estimates\n",
    "    for ngram in ngram_counts.keys():\n",
    "        if not ngram_counts[ngram] or len(ngram) < 2: continue\n",
    "\n",
    "        qd_est[ngram] = (ngram_counts[ngram] - beta) / (ngram_counts[ngram[:-1]] or ngram_counts[('STOP',)])\n",
    "\n",
    "        #if ngram[:-1] == ('DT','DT'):\n",
    "        #    print qd_est[ngram], ngram, ngram[-1], ngram_counts[ngram], ngram_counts[ngram[:-1]]\n",
    "\n",
    "        alpha[ngram[:-1]] += qd_est[ngram]\n",
    "\n",
    "    # create missing mass\n",
    "    for ngram in alpha.keys():\n",
    "        alpha[ngram] = 1 - alpha[ngram]\n",
    "\n",
    "    # divide missing mass in proportion to ngram estimates\n",
    "    for i in xrange(2,2+order):\n",
    "\n",
    "        # calculate denominator (sum of either ML or qd estimates)\n",
    "        if i == 2:\n",
    "            denom_counts = \\\n",
    "            {(v,):sum([ngram_counts[w,] if ngram_counts[v,w] == 0\\\n",
    "                       else 0 for w in tag_set]) for v in tag_set}\n",
    "        else:\n",
    "            tag_set_list = [tag_set - {'STOP',}] * (i-2) + [tag_set]\n",
    "            denom_counts = \\\n",
    "            {ngram:sum([qd_est[ngram] if ngram_counts[ngram+tuple(w)] == 0\\\n",
    "                        else 0 for w in tag_set]) for ngram in product(*tag_set_list)}\n",
    "\n",
    "        # calculate values for unseen ngrams\n",
    "        tag_set_list = [tag_set - {'STOP',}] * (i-1) + [tag_set]\n",
    "        for ngram in product(*tag_set_list):\n",
    "\n",
    "            first = ngram[:-1]\n",
    "            last = ngram[1:]\n",
    "\n",
    "            if not ngram_counts[ngram] and i == 2:\n",
    "                qd_est[ngram] = alpha[first] * ngram_counts[last] / denom_counts[first]\n",
    "\n",
    "            elif not ngram_counts[ngram]:\n",
    "                qd_est[ngram] = alpha[first] * qd_est[last] / denom_counts[first]\n",
    "\n",
    "    return qd_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find best beta value by testing on development data\n",
    "results_discounted = []\n",
    "for i in xrange(1,10):\n",
    "    qd_est = get_discounted_estimates(ngram_counts, tag_set, i/10)\n",
    "    result = score_tagger('dev_data.txt', run_viterbi, qd_est, eml_est, tag_set, True)\n",
    "    results_discounted.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best results with $\\beta=0.1$. With our beta value chosen, we'll now score the tagger on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with discounted ML estimates\n",
    "qd_est = get_discounted_estimates(ngram_counts, tag_set, 0.1)\n",
    "tagger_3 = score_tagger('test.txt', run_viterbi, qd_est, eml_est, tag_set, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tagger        | F-Score       | Precision  | Recall  |\n",
    "| ------------- |------------:| ----:|----:|\n",
    "| Baseline      | 89.16% | 80.44% |100.0%|\n",
    "| Maximum Likelihood Estimates  | 6.27%      |  98.24% |3.24%|\n",
    "| Maximum Likelihood Estimates w/ pseudo-words      | 86.90% | 92.71% |81.77%|\n",
    "| Discounted ML Estimates w/ pseudo-words  | 95.11%      |   92.70% |97.64%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final tagger does fairly well, achieving an F-score of **95.11%**. Around 98% of words are assigned a tag, and 93% of those tags are correct. As a point of reference, the Stanford Tagger 2.0 (the current state-of-the-art) has an F-score just a few points higher, at 97.32%. So not too bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
